{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9936041,"sourceType":"datasetVersion","datasetId":6108360}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Тут были эксперименты на разные модельки, которые нам не подошли.","metadata":{}},{"cell_type":"markdown","source":"При экспериментах с YOLOv8 и YOLOv10, стало очевидно, что эти модели значительно превосходят по метрикам и скорости работы такие системы, как RetinaNet и EfficientDet, что делает дальнейшую работу над ними практически бессмысленной, особенно с ограниченными квотами. Были предприняты попытки использовать PP-YOLOE, однако возникли проблемы с версиями и путями к базовым директориям, что затруднило реализацию. В результате, текущее состояние технологий указывает на необходимость сосредоточиться на более эффективных решениях, таких как предобученная в doclayout YOLOv10.\n\n\nХотя RetinaNet имеет потенциал для борьбы с дисбалансом классов (например, в документах часто преобладает класс \"paragraph\"), использование YOLOv10 позволяет добиться более стабильных результатов благодаря его оптимизированной архитектуре и способности к быстрой обработке. Поэтому, учитывая ограниченные ресурсы, целесообразнее сосредоточиться на использовании современных предобученных моделей для достижения лучших результатов.","metadata":{}},{"cell_type":"markdown","source":"Достаточно взглянуть на первые эпохи:   \n\nRetinaNet (time $ \\approx 80 $ мин) \nF1 Score: 0.3781\r\nMean IoU: 0.690\n\nYOLOv8x  (time $ \\approx 14 $ мин)\nF1 Score: $ \\approx 0.84 $\nMean IoU: $\\approx 0.9 $\n2","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import retinanet_resnet50_fpn\nimport torchvision.transforms as T\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:46:10.874472Z","iopub.execute_input":"2024-11-24T15:46:10.875164Z","iopub.status.idle":"2024-11-24T15:46:10.879639Z","shell.execute_reply.started":"2024-11-24T15:46:10.875130Z","shell.execute_reply":"2024-11-24T15:46:10.878833Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image, ImageFile\nimport torch\nimport json\nimport logging\n\n# Обработка поврежденных изображений\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# Настройка логирования\nlogging.basicConfig(level=logging.INFO)\n\nclass DocumentDataset(Dataset):\n    def __init__(self, image_paths, annotation_paths, transform=None, target_transform=None):\n        self.image_paths = sorted(image_paths)\n        self.annotation_paths = sorted(annotation_paths)\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def clip_boxes(self, boxes, image_width, image_height):\n        \"\"\"\n        Обрезает координаты боксов, чтобы они не выходили за границы изображения.\n        \"\"\"\n        boxes[:, 0] = torch.clamp(boxes[:, 0], min=0, max=image_width)\n        boxes[:, 1] = torch.clamp(boxes[:, 1], min=0, max=image_height)\n        boxes[:, 2] = torch.clamp(boxes[:, 2], min=0, max=image_width)\n        boxes[:, 3] = torch.clamp(boxes[:, 3], min=0, max=image_height)\n        return boxes\n\n    def __getitem__(self, idx):\n        try:\n            # Загружаем изображение\n            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n    \n            # Загружаем аннотацию\n            with open(self.annotation_paths[idx], 'r') as f:\n                annotation = json.load(f)\n    \n            # Формируем bounding boxes\n            boxes = torch.tensor(annotation.get(\"table\", []) +\n                                 annotation.get(\"title\", []) +\n                                 annotation.get(\"paragraph\", []) +\n                                 annotation.get(\"formula\", []) +\n                                 annotation.get(\"header\", []) +\n                                 annotation.get(\"footer\", []) +\n                                 annotation.get(\"footnote\", []) +\n                                 annotation.get(\"numbered_list\", []) +\n                                 annotation.get(\"marked_list\", []) +\n                                 annotation.get(\"table_signature\", []) +\n                                 annotation.get(\"picture_signature\", []) +\n                                 annotation.get(\"picture\", []), dtype=torch.float32)\n    \n            # Формируем метки классов\n            labels = torch.tensor([1] * len(annotation.get(\"table\", [])) +\n                                  [2] * len(annotation.get(\"title\", [])) +\n                                  [3] * len(annotation.get(\"paragraph\", [])) +\n                                  [4] * len(annotation.get(\"formula\", [])) +\n                                  [5] * len(annotation.get(\"header\", [])) +\n                                  [6] * len(annotation.get(\"footer\", [])) +\n                                  [7] * len(annotation.get(\"footnote\", [])) +\n                                  [8] * len(annotation.get(\"numbered_list\", [])) +\n                                  [9] * len(annotation.get(\"marked_list\", [])) +\n                                  [10] * len(annotation.get(\"table_signature\", [])) +\n                                  [11] * len(annotation.get(\"picture_signature\", [])) +\n                                  [12] * len(annotation.get(\"picture\", [])), dtype=torch.int64)\n    \n            # Если аннотации пусты, создаем фиктивные значения\n            if boxes.shape[0] == 0:\n                boxes = torch.tensor([[0, 0, 0, 0]], dtype=torch.float32)\n                labels = torch.tensor([0], dtype=torch.int64)\n    \n            # Проверяем границы боксов\n            boxes = self.clip_boxes(boxes, annotation[\"image_width\"], annotation[\"image_height\"])\n    \n            # Удаляем боксы с нулевой шириной или высотой\n            valid_indices = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n            boxes = boxes[valid_indices]\n            labels = labels[valid_indices]\n    \n            # Если после фильтрации боксов не осталось, создаем фиктивные значения\n            if boxes.shape[0] == 0:\n                boxes = torch.tensor([[0, 0, 0, 0]], dtype=torch.float32)\n                labels = torch.tensor([0], dtype=torch.int64)\n    \n            # Создаем словарь целей\n            targets = {\"boxes\": boxes, \"labels\": labels}\n    \n            # Преобразуем изображение\n            if self.transform:\n                image = self.transform(image)\n    \n            # Преобразуем цели, если нужно\n            if self.target_transform:\n                targets = self.target_transform(targets)\n    \n            return image, targets\n    \n        except (OSError, IOError, KeyError, json.JSONDecodeError) as e:\n            # Логирование ошибок\n            logging.warning(f\"Пропущен файл или аннотация на индексе {idx} ({self.image_paths[idx]}): {e}\")\n            # Возвращаем фиктивное значение, чтобы не нарушить DataLoader\n            dummy_image = torch.zeros(3, 224, 224)  # Замените на подходящий размер\n            dummy_targets = {\"boxes\": torch.tensor([[0, 0, 0, 0]], dtype=torch.float32),\n                             \"labels\": torch.tensor([0], dtype=torch.int64)}\n            return dummy_image, dummy_targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:51:27.968408Z","iopub.execute_input":"2024-11-24T16:51:27.969209Z","iopub.status.idle":"2024-11-24T16:51:27.985521Z","shell.execute_reply.started":"2024-11-24T16:51:27.969174Z","shell.execute_reply":"2024-11-24T16:51:27.984596Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return None, None\n    return tuple(zip(*batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:51:35.802984Z","iopub.execute_input":"2024-11-24T16:51:35.803698Z","iopub.status.idle":"2024-11-24T16:51:35.808061Z","shell.execute_reply.started":"2024-11-24T16:51:35.803664Z","shell.execute_reply":"2024-11-24T16:51:35.807080Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"images_dir = \"/kaggle/input/doclayout-raw-data/raw_data/images\"\nannotations_dir = \"/kaggle/input/doclayout-raw-data/raw_data/jsons\"\n\nimage_paths = sorted([os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.png')])\nannotation_paths = sorted([os.path.join(annotations_dir, f) for f in os.listdir(annotations_dir) if f.endswith('.json')])\n\n# Разделение данных\ntrain_images, val_images, train_annotations, val_annotations = train_test_split(\n    image_paths, annotation_paths, test_size=0.1, random_state=42\n)\n\ntransform = T.ToTensor()\ntrain_dataset = DocumentDataset(train_images, train_annotations, transform=transform)\nval_dataset = DocumentDataset(val_images, val_annotations, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:51:36.710204Z","iopub.execute_input":"2024-11-24T16:51:36.710950Z","iopub.status.idle":"2024-11-24T16:51:36.800815Z","shell.execute_reply.started":"2024-11-24T16:51:36.710896Z","shell.execute_reply":"2024-11-24T16:51:36.799876Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:54:47.371985Z","iopub.execute_input":"2024-11-24T16:54:47.372794Z","iopub.status.idle":"2024-11-24T16:54:47.377257Z","shell.execute_reply.started":"2024-11-24T16:54:47.372762Z","shell.execute_reply":"2024-11-24T16:54:47.376402Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = retinanet_resnet50_fpn(pretrained=True)\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:54:48.938693Z","iopub.execute_input":"2024-11-24T16:54:48.939060Z","iopub.status.idle":"2024-11-24T16:54:49.573955Z","shell.execute_reply.started":"2024-11-24T16:54:48.939028Z","shell.execute_reply":"2024-11-24T16:54:49.573147Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, optimizer, device, num_epochs):\n    \"\"\"\n    Обучение модели на нескольких эпохах.\n    \"\"\"\n    model.to(device)\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        # Режим обучения\n        model.train()\n        train_loss = 0\n        for images, targets in tqdm(train_loader, desc=\"Training\"):\n            if images is None or targets is None:\n                continue  # Пропускаем пустые батчи\n            \n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            optimizer.zero_grad()\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            losses.backward()\n            optimizer.step()\n            train_loss += losses.item()\n        \n        train_losses.append(train_loss / len(train_loader))\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, targets in tqdm(val_loader, desc=\"Validation\"):\n                if images is None or targets is None:\n                    continue  # Пропускаем пустые батчи\n\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                val_loss += losses.item()\n\n        val_losses.append(val_loss / len(val_loader))\n        print(f\"Epoch {epoch + 1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n\n    return train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:54:52.036468Z","iopub.execute_input":"2024-11-24T16:54:52.036790Z","iopub.status.idle":"2024-11-24T16:54:52.046494Z","shell.execute_reply.started":"2024-11-24T16:54:52.036759Z","shell.execute_reply":"2024-11-24T16:54:52.045657Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"Эта ячейка была перезапущена, но ранее она обучалась около 80 минут:","metadata":{}},{"cell_type":"code","source":"num_epochs = 1\ntrain_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, device, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:54:53.597508Z","iopub.execute_input":"2024-11-24T16:54:53.598298Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"Training:  11%|█         | 481/4518 [06:07<51:05,  1.32it/s] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, f1_score\nfrom torchvision.ops import box_iou\nimport numpy as np\n\ndef evaluate_model(model, data_loader, device, iou_threshold=0.5):\n    \"\"\"\n    Оценивает метрики модели: precision, F1 и mean IoU.\n\n    Аргументы:\n    - model: обученная модель.\n    - data_loader: DataLoader для оценки (валидационный или тестовый набор).\n    - device: устройство (CPU или GPU).\n    - iou_threshold: порог IoU для определения True Positive.\n\n    Возвращает:\n    - mean_precision: средняя точность (precision).\n    - mean_f1: средний F1-score.\n    - mean_iou: средний IoU.\n    \"\"\"\n    model.eval()\n    all_precisions, all_f1s, all_ious = [], [], []\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n\n            for i, output in enumerate(outputs):\n                # Истинные рамки и классы\n                true_boxes = targets[i][\"boxes\"].to(device)\n                true_labels = targets[i][\"labels\"].to(device)\n\n                # Предсказанные рамки и классы\n                pred_boxes = output[\"boxes\"]\n                pred_labels = output[\"labels\"]\n                pred_scores = output[\"scores\"]\n\n                # Считаем IoU для всех предсказанных рамок с истинными\n                ious = box_iou(pred_boxes, true_boxes)\n\n                # Определяем True Positive, False Positive, False Negative\n                true_positive = (ious.max(dim=1)[0] >= iou_threshold).sum().item()\n                false_positive = len(pred_boxes) - true_positive\n                false_negative = len(true_boxes) - true_positive\n\n                # Precision, Recall, F1\n                precision = true_positive / (true_positive + false_positive + 1e-6)\n                recall = true_positive / (true_positive + false_negative + 1e-6)\n                f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n\n                # Средний IoU\n                mean_iou = ious[ious >= iou_threshold].mean().item() if ious.numel() > 0 else 0.0\n\n                # Добавляем метрики в список\n                all_precisions.append(precision)\n                all_f1s.append(f1)\n                all_ious.append(mean_iou)\n\n    # Возвращаем средние метрики\n    return {\n        \"precision\": np.mean(all_precisions),\n        \"f1\": np.mean(all_f1s),\n        \"mean_iou\": np.mean(all_ious),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T06:37:35.189114Z","iopub.execute_input":"2024-11-28T06:37:35.189855Z","iopub.status.idle":"2024-11-28T06:37:38.503539Z","shell.execute_reply.started":"2024-11-28T06:37:35.189810Z","shell.execute_reply":"2024-11-28T06:37:38.502813Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"metrics = evaluate_model(model, val_loader, device)\n\nprint(f\"Precision: {metrics['precision']:.4f}\")\nprint(f\"F1 Score: {metrics['f1']:.4f}\")\nprint(f\"Mean IoU: {metrics['mean_iou']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:29:43.488867Z","iopub.execute_input":"2024-11-24T18:29:43.489415Z","iopub.status.idle":"2024-11-24T18:34:48.106063Z","shell.execute_reply.started":"2024-11-24T18:29:43.489378Z","shell.execute_reply":"2024-11-24T18:34:48.104895Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 502/502 [05:04<00:00,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Precision: 0.1944\nF1 Score: 0.3781\nMean IoU: 0.6902\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":61}]}